{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  What is Data Science?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science is the process of using data to understand different things. Data science is the field of exploring, manipulating and analyzing data. Using the data to answer questions or make recommendations.\n",
    "\n",
    "Data science is the study of large quantities of data, which can reveal insights that help organizations make strategic choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science can help organizations understand their environments, analyze existing issues, and reveal previously hidden opportunities. Data scientists use data analysis to add to the knowledge of the organization by investigating data, exploring the best way to use it to provide value to the business.\n",
    "\n",
    "Many organizations will use data science to focus on a specific problem, and so it's essential to clarify the question that the organization wants answered. This first and most crucial step defines how the data science project progresses.\n",
    "\n",
    "The next questions are: \"what data do we need to solve the problem, and where will that data come from?\". Data scientists can analyze structured and unstructured data from many sources,  and depending on the nature of the problem, they can choose to analyze the data in different ways. Using multiple models to explore the data reveals patterns and outliers.\n",
    "\n",
    "When the data has revealed its insights, the role of the data scientist becomes that of a storyteller, communicating the results to the project stakeholders. Data scientists can use powerful data visualization tools to help stakeholders understand the nature of the results, and the recommended action to take.\n",
    "\n",
    "A data scientist must be curious, judgemental and argumentative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizations can leverage the almost unlimited amount of data now available to them in a growing number of ways. However, all organizations ultimately use data science for the same reason—to discover optimum solutions to existing problems.\n",
    "\n",
    "How do you get a better solution that is efficient? You must: Identify the problem and establish a clear understanding of it. Gather the data for analysis. Identify the right tools to use. Develop a data strategy. Case studies are also helpful in customizing a potential solution. Once these conditions exist and available data is extracted, you can develop a machine learning model. It will take time for an organization to refine best practices for data strategy using data science, but the benefits are worth it.\n",
    "\n",
    "So structured data is more like tabular data things that you’re familiar with in Microsoft Excel format. You've got rows and columns and that's called structured data. Unstructured data is basically data that is coming from mostly from web where it's not tabular. It is not, it's not in rows and columns. It's text. It's sometimes it's video and audio, so you would have to deploy more sophisticated algorithms to extract data. Regression tells you what the base fare is and what is the relationship between time and the fare you have paid, and the distance you have traveled and the fare you've paid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who is a Data Scientist? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data scientist is someone who finds solutions to problems by analizing big or small data using appropiate tools and communicates the findings to the relevant stakeholders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Big Data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no one definition of Big data. Ernst and Young defines big data as: “Big Data refers to the dynamic, large and disparate volumes of data being created by people, tools, and machines. It requires new, innovative, and scalable technology to collect, host, and analytically process the vast amount of data gathered in order to derive real-time business insights that relate to consumers, risk, profit, performance, productivity management, and enhanced shareholder value.” \n",
    "\n",
    "The term big data refers to data sets that are so massive, so quickly built, and so varied that they defy traditional analysis methods such as you might perform with a relational database. The concurrent development of enormous compute power in distributed networks and new tools and techniques for data analysis means that organizations now have the power to analyze these vast data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The V's of Big Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Velocity is the speed at which data accumulates. Data is being generated extremely fast, in a process that never stops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volume is the scale of the data, or the increase in the amount of data stored. Drivers of volume are the increase in data sources, higher resolution sensors, and scalable infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variety "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variety is the diversity of the data. Structured data fits neatly into rows and columns, in relational databases while unstructured data is not organized in a pre-defined way, like Tweets, blog posts, pictures, numbers, and video. Variety also reflects that data comes from different sources, machines, people, and processes, both internal and external to organizations. Drivers are mobile technologies, social media, wearable technologies, geo technologies, video, and many, many more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Veracity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veracity is the quality and origin of data, and its conformity to facts and accuracy. Attributes include consistency, completeness, integrity, and ambiguity. Drivers include cost and the need for traceability. With the large amount of data available, the debate rages on about the accuracy of data in the digital era. Is the information real, or is it false? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value is our ability and need to turn data into value. Value isn't just profit. It may have medical or social benefits, as well as customer, employee, or personal satisfaction. The main reason that people invest time to understand Big Data is to derive value from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Data Mining?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data mining is the process of automatically searching and analyzing data, discovering previously unrevealed patterns. It involves preprocessing the data to prepare it and transforming it into an appropriate format. Once this is done, insights and patterns are mined and extracted using various tools and techniques ranging from simple data visualization tools to machine learning and statistical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is a subset of AI that uses computer algorithms to analyze data and make intelligent decisions based on what it is learned without being explicitly programmed. Machine learning algorithms are trained with large sets of data and they learn from examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Deep Learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is a specialized subset of machine learning that uses layered neural networks to simulate human decision-making. Deep learning algorithms can label and categorize information and identify patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Neural Networks? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Artificial neural networks, often referred to simply as neural networks, take inspiration from biological neural networks, although they work quite a bit differently. A neural network in AI is a collection of small computing units called neurons that take incoming data and learn to make decisions over time. Neural networks are often layer-deep and are the reason deep learning algorithms become more efficient as the data sets increase in volume, as opposed to other machine learning algorithms that may plateau as data increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Intelligence and Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Science is the process and method for extracting knowledge and insights from large volumes of disparate data. It's an interdisciplinary field involving mathematics, statistical analysis, data visualization, machine learning, and more. Data Science can use many of the AI techniques to derive insight from data.\n",
    "\n",
    " Data Science is a broad term that encompasses the entire data processing methodology while AI includes everything that allows computers to learn how to solve problems and make intelligent decisions. Both AI and Data Science can involve the use of big data. That is, significantly large volumes of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Languages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python, R and SQL are the most recommended languages for data science. Other languages like Scala, Java, C++, Julia, Javascript, PHP, Go, ruby and Visual Basic are mostly used as well.\n",
    "\n",
    "The language choose depends on the things you need to accomplish and the problem needed to be solved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its by far the mos popular programming language for data science. Python uses clear readable syntax and with less code.\n",
    "\n",
    "For data science, python has scientific computing libraries like Pandas, Numpy, Scipy and Matplotlib. For artificital intelligence it has Pytorch, Tensorflow, Keras and Scikit-learn. For Natural Language Processing (NLP) uses the Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Python, R is free to use. Its most often used by statisticians, mathematicians and data miners for developing statistical software, graphing and data analysis. The array-oriented syntax makes it easier to translate from math to code.\n",
    "\n",
    "It has become the world's largest repository of statistical knowledge. R integrates well with other computer languages like C++, Java, C, .Net and Python. Common mathematical operations like matrix multiplication work straight out of the box. It has stronger object-oriented programming facilities than most statistical computing languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL or \"Structured Query Language\" is not considered to be like other development languages because it's a non-procedual language and its scope is limited to querying and managing data. SQL is useful in handling structured data. the data incorporating relations among entities and variables.\n",
    "\n",
    "SQL was designed for managing data in relational databases. knowing SQL will help you get jobs as a business and data analyst and is a must in data engineering and data science. When performing operations with SQL the data is accessed directly. This speeds up workflow executions. SQL is the interpreter between you and the database. \n",
    "Learning SQL and using it in one database, will easily apply your SQL knowledge with many others databases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Categories of Data Science Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data Management:** is the process of persisting and retrieving data.\n",
    "- **Data integration and Transformation:** refers to as extract, transform and load or (ETL) is the process of retrieving data from remote data management systems.\n",
    "- **Data Visualization:** Is part of an initial data exploration process as well as being part of a final deliverable.\n",
    "- **Model Building:** Process of creating a machine learning or deep learning model using an appropiate algorithm with a lot of data.\n",
    "- **Model Deployment:** makes such a machine learning or deep learning model available to third-party applications.\n",
    "- **Model monitoring and Assessment:** ensures continuous performance quality checks on the deployed models.\n",
    "    all these are for accuracy, fairness and adversarial robustness.\n",
    "- **Code Asset Management:** Uses versioning and other collaborative features to facilitate teamwork. \n",
    "- **Data Asset Management:** Brings the same versioning and collaborate components to data. Also supports replication, backup and access right management. \n",
    "- **Development Environments:** commonly known as Integrated Development Environments or \"IDEs\" are tools that help data scientist to implement, execute, test and deploy their work. \n",
    "- **Execution Environments:** are tools where data preprocessing, model training and deployment take place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CRISP-DM methodology is a process aimed at increasing the use pf data mining over a wide variety of business applicationes and industries. The intent is to take scenarios and general behaviors to make them domain neutral. \n",
    "Its implemented in six steps in order to have a reasonable chance of success.\n",
    "\n",
    "1. **Business Understanding:** In this stage  the intention of the project is outlined. It requires communication and clarity. The difficulty here is that stakeholders have different objectives, biases, and modalities of relating information. They don’t all see the same things or in the same manner. Without clear, concise, and complete perspective of what the project goals are resources will be needlessly expended.\n",
    "\n",
    "    It helps clarify the goal of the entity asking the question.\n",
    "\n",
    "2. **Data Understanding:** The understanding of what the business wants and needs will determine what data is collected, from what sources, and by what methods.\n",
    "\n",
    "3. **Data Preparation:** Once the data has been collected, it must be transformed into a useable subset unless it is determined that more data is needed. Once a dataset is chosen, it must then be checked for questionable, missing, or ambiguous cases.\n",
    "\n",
    "4. **Modeling:** Once prepared, the data must be expressed through whatever appropriate models, give meaningful insights, and hopefully new knowledge.\n",
    "\n",
    "    This is the purpose of data mining: to create knowledge information that has meaning and utility. The use of models reveals patterns and structures within the data that provide insight into the features of interest. Models are selected on a portion of the data and adjustments are made if necessary.\n",
    "    \n",
    "5. **Evaluation:** The selected model must be tested. This is usually done by having a pre-selected test, set to run the trained model on. This will allow you to see the effectiveness of the model on a set it sees as new. Results from this are used to determine efficacy of the model and foreshadows. \n",
    "\n",
    "6. **Deployment:** The model is used on new data outside of the scope of the dataset and by new stakeholders. The new interactions at this phase might reveal the new variables and needs for the dataset and model.\n",
    "\n",
    "CRISP-DM is a highly **flexible** and **cyclical model**. Flexibility is required at each step along with communication to keep the project on track. At any of the six stages, it may be necessary to revisit an earlier stage and make changes. The key point of this process is that it’s cyclical; therefore, even at the finish you are having another business understanding encounter to discuss the viability after deployment. *The CRISP-DM flowchart is highly iterative and repetitive (it never ends).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = \"1934567\"\n",
    "print(A[1::2])\n",
    "str(1+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
